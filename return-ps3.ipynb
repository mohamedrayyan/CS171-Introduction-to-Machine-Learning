{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Problem Set 3\n",
    "# Due Sunday, November 22, 2020\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "In this problem set, you are to implement a three-layer (3 layers of weights, 2 hidden layers of units) neural network for binary classification.  All non-linearities are to be sigmoids.\n",
    "\n",
    "Details are given below.  *Please read the **entire** notebook carefully before proceeding.*\n",
    "\n",
    "You need to both fill in the necesary code, **and** answer the question at the bottom.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your information below:\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFCCCC\">\n",
    "    Your Name (submitter): Mohamed Rayyan<br>\n",
    "Your student ID (submitter): 862037325\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFCCFF\">\n",
    "    Total Grade: 14/30<br>\n",
    "    Late Days on this assigment: 0<br>\n",
    "    Total Late Days so far: <br>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the only imports that are necessary (or allowed)\n",
    "import numpy as np\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "We will be using a USPS digit dataset (provided in the file uspsall73.mat).\n",
    "It has 16-by-16 grayscale images of each of the 10 different hand-written digits\n",
    "However, we will load only two of the digits to use as the two classes in\n",
    "binary classification\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load two of the 10 classes (c1 is for Y=+1 and c2 is for Y=0)\n",
    "# Note that for neural networks, we will be using Y={+1,0} instead of Y={+1,-1}\n",
    "def loaddigitdata(c1,c2,m):\n",
    "    f = h5py.File('uspsall73.mat','r') \n",
    "    data = f.get('data') \n",
    "    data = np.array(data).astype(float)\n",
    "    X = np.concatenate((data[c1,:,:],data[c2,:,:]))\n",
    "    Y = np.concatenate((np.zeros((data.shape[1])),np.ones((data.shape[1]))))\n",
    "    \n",
    "    rndstate = np.random.get_state() # going to set the \"random\" shuffle random seed\n",
    "    np.random.seed(132857) # setting seed so that dataset is consistent\n",
    "    p = np.random.permutation(X.shape[0])\n",
    "    X = X[p] # this and next line make copies, but that's okay given how small our dataset is\n",
    "    Y = Y[p]\n",
    "    np.random.set_state(rndstate) # reset seed\n",
    "    \n",
    "    trainX = X[0:m,:] # use the first m (after shuffling) for training\n",
    "    trainY = Y[0:m,np.newaxis]\n",
    "    validX = X[m:,:] # use the rest for validation\n",
    "    validY = Y[m:,np.newaxis]\n",
    "    return (trainX,trainY,validX,validY)\n",
    "\n",
    "# In case you care (not necessary for the assignment)\n",
    "def drawexample(x,ax=None): # takes an x *vector* and draws the image it encodes\n",
    "    if ax is None:\n",
    "        plt.imshow(np.reshape(x,(16,16)).T,cmap='gray')\n",
    "    else:\n",
    "        ax.imshow(np.reshape(x,(16,16)).T,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data, to differentiate between 7s and 9s\n",
    "# we will use on 1100 examples for training (50% of the data) and the other half for validation\n",
    "(trainX,trainY,validX,validY) = loaddigitdata(6,8,1100)\n",
    "means = trainX.mean(axis=0)\n",
    "stddevs = trainX.std(axis=0)\n",
    "stddevs[stddevs<1e-6] = 1.0\n",
    "trainX = (trainX-means)/stddevs # z-score normalization\n",
    "validX = (validX-means)/stddevs # apply same transformation to validation set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert this cell to a code cell if you wish to see each of the examples, plotted\n",
    "# (completely not necessary for the problem set)\n",
    "f = plt.figure()\n",
    "f.set_size_inches(8,8)\n",
    "\n",
    "ax = f.add_subplot(111)\n",
    "plt.ion()\n",
    "f.canvas.draw()\n",
    "for exi in range(trainX.shape[0]):\n",
    "    display.clear_output(wait=True)\n",
    "    drawexample(trainX[exi,:])\n",
    "    digitid = (9 if trainY[exi]>0 else 7)\n",
    "    ax.set_title('y = '+str(int(trainY[exi]))+\" [\"+str(digitid)+\"]\")\n",
    "    display.display(f)\n",
    "    #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITE `nneval` and `trainneuralnet` [25 points]\n",
    "\n",
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "This is the main portion of the assignment\n",
    "\n",
    "Note that the $Y$ values are +1 and 0 (not +1 and -1).  This is as in class for neural networks and works better with a sigmoid output.\n",
    "\n",
    "You need to write the two functions below (plus any more you would like to add to help): `nneval` and `trainneuralnet`.  The first takes an array/matrix of X vectors and the weights from a neural network and returns a vector of predicted Y values (should be numbers between 0 and 1 -- the probability of class +1, for each of the examples).  The second takes a data set (Xs and Ys), the number of hidden units, and the lambda value (for regularization), and returns the weights.  W1 are the weights from the input to the hidden and W2 are the weights from the hidden to the output.\n",
    "\n",
    "A few notes:\n",
    "- **Starting Weights**: The code supplied randomly selects the weights near zero.  https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79 has a reasonable explanation of why we are doing it this way.  But for the purposes of the assignment, you can just accept this is a good way to initialize neural network weights.\n",
    "- **Offset Terms**: Each layer should have an \"offset\" or \"intercept\" unit (to supply a 1 to the next layer), except the output layer.\n",
    "- **Batch Updates**: For a problem this small, use batch updates.  That is, the step is based on the sum of the gradients for each data point in the training set.\n",
    "- **Step Size**: http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms describes a number of methods to adaptively control $\\eta$ for fast convergence.  You don't need to understand any of them; however, without them, convergence to good solutions on this problem can be quite slow.  Therefore, *use RMSprop*: the code below has a simple version of RMSprop that is sufficient for this assignment.  You need to supply the code that calculates `sumofgrad2` which should be the sum of the square of each element of the gradient (the squared length of the gradient).  (for debugging, feel free to use a constant $\\eta$). \n",
    "- **Stopping Criterion**: To determine when to stop, check the loss function every 10 iterations.  If it has not improved by at least $10^{-6}$ over those 10 iterations, stop.\n",
    "- **Regularization**: You should penalize (from the regularization) all of the weights, even those coming out of offset units.  While it makes sense sometimes not to penalize the ones for the constant $1$ units, you'll find this easier if you just penalize them all.\n",
    "\n",
    "Tips that might help:\n",
    "- Display the loss function's value every 10 iterations (or so).  It should be getting smaller.  If not, your gradient is not pointing in the right direction.\n",
    "- The smaller $\\lambda$ is and the more units, the more difficult (longer) the optimization will be.\n",
    "- Write a function to do forward propagation and one to do backward propagation.  Write a function to evaluate the loss function.  In general, break things up to keep things straight.\n",
    "- Processing the entire batch at once is more efficient in numpy.  Use numpy broadcasting to avoid loops where possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFCCFF\">\n",
    "    Grade: 12/25<br>\n",
    "    Comments:\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEEL FREE TO ADD HELPER FUNCTIONS\n",
    "def sigmoid(a):\n",
    "    return np.array([(1 /(1+np.exp(-a)))])\n",
    "\n",
    "# Wts is whatever object you return from trainneuralnet\n",
    "def nneval(X,Wts):\n",
    "    # YOUR CODE HERE\n",
    "    nn =dict()\n",
    "    Y =[]\n",
    "    for i in range(len(X)):\n",
    "        nn[i] =dict()\n",
    "        inputt =X[i]\n",
    "        inputt =np.insert(inputt,0,1)\n",
    "        for k in range(len(Wts)):\n",
    "            nn[i][k] =dict()\n",
    "            nn[i][k]['z'] =np.matmul(inputt,Wts[k].T)\n",
    "            nn[i][k]['a'] =sigmoid(nn[i][k]['z'])\n",
    "            nn[i][k]['a'] =np.insert(nn[i][k]['a'], 0, 1)\n",
    "            inputt =nn[i][k]['a']\n",
    "        \n",
    "        Y.append(nn[i][2]['a'][1])\n",
    "        \n",
    "    # Should return a vector of values (btwn 0 and 1) for each of the rows of X\n",
    "    # print(Y)\n",
    "    return 1-np.array(Y).T\n",
    "    \n",
    "    \n",
    "def losss(X, Y):\n",
    "    return((-(Y * np.log(X) + (1 - Y) * np.log(1-X))))\n",
    "\n",
    "def summ(Wts):\n",
    "    return(np.sum(Wts[0] ** 2) + np.sum(Wts[1] ** 2) + np.sum(Wts[2] ** 2))\n",
    "    \n",
    "def evaluateLoss(losses, lossVal):\n",
    "    losses.append(lossVal)\n",
    "    if(len(losses) > 10):\n",
    "        losses.pop(0)\n",
    "    \n",
    "    threshold = 10 **(-6)\n",
    "    \n",
    "    return(not (abs(losses[0] - lossVal) < threshold) and (losses[0] > lossVal))\n",
    "    \n",
    "# Your functions need only work for neural networks of exactly 3 layers of weights\n",
    "# This training function has a single scalar parameter, nhid, to indicate the number of\n",
    "# hidden units.  This is the number in the first hidden layer.  The second hidden layer will have 1/2 this number\n",
    "# You can use \"printinfo\" to control whether to print out debugging info (or you can just ignore it)\n",
    "def trainneuralnet(X,Y,nhid,lam,printinfo=False):\n",
    "    # The number of examples (m) and number of input dimensions (n) of the input data\n",
    "    (m,n) = X.shape\n",
    "    \n",
    "    # This is the code that initializes the weigth matrics:\n",
    "    # W1 is nhid by n+1 (to map from the input, plus a constant term, to the first hidden layer)\n",
    "    # W2 is nhid/2 by nhid+1 (to map from the first hidden layer of units to the second)\n",
    "    # W3 is nhid/2+1 by 1 (to map from the second layers of hidden units to the output)\n",
    "    W1 = (np.random.rand(nhid,n+1)*2-1)*np.sqrt(6.0/(n+nhid+1)) # weights to each hidden unit from the inputs (plus the added offset unit)\n",
    "    W2 = (np.random.rand(nhid//2,nhid+1)*2-1)*np.sqrt(6.0/(nhid+nhid/2+1))\n",
    "    W3 = (np.random.rand(1,nhid//2+1)*2-1)*np.sqrt(6.0/(nhid+2)) # weights to the single output unit from the hidden units (plus the offset unit)\n",
    "    W2[:,0] = -nhid/2.0\n",
    "    W3[:,0] = -nhid/4.0\n",
    "    \n",
    "    Wts = [W1,W2,W3] # I put them together in a list, but you can use any structure to keep them together and return them in the end\n",
    "    \n",
    "    Eg2=1\n",
    "    # Your loop here:\n",
    "    nn =dict()\n",
    "    loss =0\n",
    "    lwts =0\n",
    "    lssa =[100]\n",
    "    count =0\n",
    "    sumofgrad2 =0\n",
    "    while(evaluateLoss(lssa, lwts) or (count < 12)):\n",
    "        lwts =0\n",
    "        for i in range(m):\n",
    "            nn[i] =dict()\n",
    "            inputt =X[i]\n",
    "            inputt =np.insert(inputt,0,1)\n",
    "            for k in range(len(Wts)):\n",
    "                nn[i][k] =dict()\n",
    "                nn[i][k]['z'] =np.matmul(inputt,Wts[k].T)\n",
    "                nn[i][k]['a'] =sigmoid(nn[i][k]['z'])\n",
    "                nn[i][k]['a'] =np.insert(nn[i][k]['a'], 0, 1)\n",
    "                inputt =nn[i][k]['a']\n",
    "\n",
    "            nn[i][2]['s'] =(nn[i][2]['a'] -Y[i])\n",
    "            nn[i][2]['s'] =np.delete(nn[i][2]['s'],0)\n",
    "\n",
    "            d =nn[i][2]['s'] *Wts[2]\n",
    "            s =d*nn[i][1]['a']\n",
    "            s =s*(1-nn[i][1]['a'])\n",
    "            nn[i][1]['s'] =np.delete(s,0,1)\n",
    "\n",
    "            d =np.matmul(nn[i][1]['s'],np.delete(Wts[1],0,1))\n",
    "            s =d *nn[i][0]['a'][1:]\n",
    "            s =s *(1-nn[i][0]['a'][1:])\n",
    "            nn[i][0]['s'] =s\n",
    "\n",
    "            g2 =np.array([nn[i][2]['s'] *nn[i][1]['a']])\n",
    "            sumofgrad2 =np.sum(g2**2)\n",
    "            g1 =np.array(nn[i][1]['s'].T *nn[i][0]['a'])\n",
    "            sumofgrad2 =sumofgrad2 +np.sum(g1**2)\n",
    "            g0 =np.array(nn[i][0]['s'].T *np.insert(X[i],0,1))\n",
    "            sumofgrad2 =sumofgrad2 +np.sum(g0**2)\n",
    "\n",
    "            # in the loop, after calculating the gradient, but before making a step\n",
    "            # calculate the sum of the squares of all of the gradient values\n",
    "            # and store it in sumofgrad2\n",
    "            # then execute this code to get the step size, eta:\n",
    "            Eg2 = 0.9*Eg2 + 0.1*sumofgrad2\n",
    "            eta = 0.01/(np.sqrt((1e-10+Eg2)))\n",
    "\n",
    "            Wts[0] =Wts[0] -(eta*g0)\n",
    "            Wts[1] =Wts[1] -(eta*g1)\n",
    "            Wts[2] =Wts[2] -(eta*g2)\n",
    "\n",
    "            lwts = lwts +  losss(nn[i][2]['a'][1],Y[i])\n",
    "        \n",
    "        lwts =(((1/m) * lwts + (lam/m) * summ(Wts))[0])\n",
    "        \n",
    "        count =count +1\n",
    "\n",
    "#         print(lssa)\n",
    "        \n",
    "#             print(count)\n",
    "#             print(lssa, \" | \", abs(lssa[0] - lssa[-1]), \" | \", 10 ** (-6))\n",
    "\n",
    "    \n",
    "    # when done, return your weights\n",
    "    return Wts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.44 s, sys: 986 µs, total: 5.44 s\n",
      "Wall time: 5.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use this cell (or others you add) to check your network\n",
    "# I would debug on simple examples you create yourself (trying to understand what happens with\n",
    "#  the full 256-dimensional data is hard)\n",
    "\n",
    "#an example of training on the USPS data with 32/16 hidden units and lambda=0.1, takes about 9800 iterations and about 50 seconds for the solutions\n",
    "Wts = trainneuralnet(trainX,trainY,32,0.1,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance plot\n",
    "<div class=\"alert alert-info\">\n",
    "The code below will plot your algorithm's error rate on this data set for various regularization strengths and numbers of hidden units.\n",
    "\n",
    "Make sure your code works for this plot.\n",
    "\n",
    "My code runs in about 12 minutes (to produce the full plot below)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 51s, sys: 476 ms, total: 8min 51s\n",
      "Wall time: 8min 51s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# This code is given.  Do not modify.\n",
    "def setupfig():\n",
    "    f = plt.figure()\n",
    "    f.set_size_inches(8,8)\n",
    "    ax = f.add_subplot(111)\n",
    "    plt.ion()\n",
    "    f.canvas.draw()\n",
    "    return (f,ax)\n",
    "\n",
    "def plotit(lams,nhiddens,erates,f,ax):\n",
    "    ax.clear()\n",
    "    for i in range(nhiddens.shape[0]):\n",
    "        ax.plot(lams,erates[:,i],'*-')\n",
    "    ax.set_yscale('log',subs=[1,2,3,4,5,6,7,8,9])\n",
    "    ax.set_yticks([0.1,0.01])\n",
    "    ax.set_xscale('log')\n",
    "    f.canvas.draw()\n",
    "    ax.set_xlabel('lambda')\n",
    "    ax.set_ylabel('validation error rate')\n",
    "    ax.legend([(('# hidden units = '+str(x)) if x>0 else 'logistic regression') for x in nhiddens])\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "def errorrate(Y,predy):\n",
    "    predy[predy<0.5] = 0.0\n",
    "    predy[predy>=0.5] = 1.0\n",
    "    return (predy!=Y).mean()\n",
    "    \n",
    "def multirestart(trainX,trainY,nhid,lam,ntries):\n",
    "    besterrsc = 1.0\n",
    "    for i in range(ntries):\n",
    "        Wts = trainneuralnet(trainX,trainY,nhid,lam)\n",
    "        errsc = errorrate(trainY,nneval(trainX,Wts))\n",
    "        if errsc<besterrsc:\n",
    "            returnWts = Wts\n",
    "            besterrsc = errsc\n",
    "    return returnWts\n",
    "    \n",
    "nhiddens = np.array([0,4,8,16])\n",
    "nhiddens = np.array([2,8,18,32])\n",
    "lams = np.logspace(-2.5,1.5,8)\n",
    "erates = np.empty([lams.shape[0],nhiddens.shape[0]])\n",
    "erates[:,:] = np.nan\n",
    "\n",
    "(f,ax) = setupfig()\n",
    "\n",
    "    \n",
    "for ni, nhid in enumerate(nhiddens):\n",
    "    for li, lam in reversed(list(enumerate(lams))):\n",
    "        if nhid==0:\n",
    "            w = learnlogreg(trainX,trainY,lam)\n",
    "            predy = predictlogreg(validX,w)[:,np.newaxis]\n",
    "        else:\n",
    "            Wts = multirestart(trainX,trainY,nhid,lam,1) #trainneuralnet(trainX,trainY,nhid,lam)\n",
    "            predy = nneval(validX,Wts)\n",
    "        erates[li,ni] = errorrate(validY,predy)\n",
    "        \n",
    "        plotit(lams,nhiddens,erates,f,ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTERPRET the Plot [5 points]\n",
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "How do you interpret the plot above?  How and why does the plot differ by number of hidden units?  By $\\lambda$ value?  What parts of this plot agree with the material taught?  What parts do not?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFCCFF\">\n",
    "    Grade: 2/5<br>\n",
    "    Comments: Plot is wrong, explanation has errors. smaller lambda does not neccesarily equate to more accuracy.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answer Here\n",
    "\n",
    "The plot above measures the performance of a neural network in terms of hideen units and the value of lambda.\n",
    "\n",
    "The more hidden units there are the more accurate the network is. This is due to more computation is occuring to better approximate the function\n",
    "\n",
    "The smaller lambda is the more accurate the network is. This is due to when lambda is smaller it would take a smaller step into the wrong direction (if it is going in the wrong direction)\n",
    "\n",
    "The part of this plot that agree with material taught in class is the smaller the lambda the better and the more hidden units there the better the network is.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
